# ğŸ“Š MaaS Grafana Dashboards

This directory contains Grafana dashboard samples for the MaaS platform.

## ğŸ“ Dashboard Files

| File | Description |
| ---- | ----------- |
| `platform-admin-dashboard.json` | Unified view for platform administrators |
| `ai-engineer-dashboard.json` | API key-filtered view for AI engineers |
| `maas-token-metrics-dashboard.json` | Legacy token metrics dashboard |

## ğŸ“– Documentation Files

| File | Description |
| ---- | ----------- |
| `METRICS-SUMMARY.md` | **Main reference** - Complete metrics documentation, queries, and limitations |
| `METRICS-EXPORT-FLOW.md` | Architecture flow showing how metrics are exported |
| `PROMETHEUS-COUNTER-BEHAVIOR.md` | Educational guide on Prometheus counter behavior |

## ğŸ¯ Available Metrics

### âœ… All Metrics Working!

| Category | Metrics | Labels |
| -------- | ------- | ------ |
| **Limitador** | `authorized_hits`, `authorized_calls`, `limited_calls`, `limitador_up` | âœ… `user`, `tier`, `model`, `limitador_namespace` |
| **Istio Gateway** | `istio_requests_total`, `istio_request_duration_milliseconds_bucket` | âœ… `response_code`, `destination_service_name` |
| **vLLM/KServe** | `vllm:num_requests_running`, `vllm:num_requests_waiting`, `vllm:gpu_cache_usage_perc`, `vllm:e2e_request_latency_seconds`, `vllm:request_inference_time_seconds` | âœ… `model_name` |
| **Kubernetes** | `kube_pod_status_phase`, `ALERTS` | âœ… `namespace`, `pod`, `alertname` |
| **Authorino** | `controller_runtime_reconcile_*` | âš ï¸ Operator metrics only |

**Verified on cluster:**

```text
authorized_hits{model="facebook-opt-125m-simulated",tier="free",user="tgitelma-redhat-com-dd264a84",...} 376
istio_requests_total{response_code="200",destination_service_name="facebook-opt-125m-simulated-kserve-workload-svc",...} 55
```

See `METRICS-SUMMARY.md` for full details and query examples.

## ğŸ”§ How to Use

1. **Automated Deployment (Recommended):**
   ```bash
   ./scripts/install-observability.sh
   ```
   This script installs Grafana, configures Prometheus datasource, and deploys all dashboards.

2. **Manual Import:**
   - Go to Grafana â†’ Dashboards â†’ Import
   - Upload the desired dashboard JSON file
   - Configure Prometheus datasource

3. **Prerequisites:**
   - User-workload-monitoring enabled in OpenShift
   - ServiceMonitors deployed for Limitador, Istio Gateway, and KServe models
   - Kuadrant policies configured with TelemetryPolicy

## ğŸ“ˆ Working Queries

```promql
# Requests per user
sum by (user) (authorized_hits)

# Requests per model
sum by (model) (authorized_hits)

# Top 10 users
topk(10, sum by (user) (authorized_hits))

# Success rate per user
sum by (user) (authorized_calls) / (sum by (user) (authorized_calls) + sum by (user) (limited_calls))

# P95 latency by service (Istio)
histogram_quantile(0.95, sum by (destination_service_name, le) (rate(istio_request_duration_milliseconds_bucket[5m])))

# Unauthorized requests (401)
sum(rate(istio_requests_total{response_code="401"}[5m]))

# Overall error rate (4xx + 5xx)
sum(rate(istio_requests_total{response_code=~"4.."}[5m])) + sum(rate(istio_requests_total{response_code=~"5.."}[5m]))

# Firing alerts in MaaS namespaces
count(ALERTS{alertstate="firing", namespace=~"llm|kuadrant-system|maas-api"})

# Model inference P95 latency (vLLM)
histogram_quantile(0.95, sum(rate(vllm:e2e_request_latency_seconds_bucket[5m])) by (le, model_name))

# Total model requests (scales with dashboard time range)
sum(increase(vllm:e2e_request_latency_seconds_count[$__range]))

# Token throughput (works with both real vLLM and simulator)
# Uses OR to support both metric naming conventions
sum(rate(vllm:prompt_tokens_total[5m])) or sum(rate(vllm:request_prompt_tokens_sum[5m]))  # Prompt tokens/s
sum(rate(vllm:generation_tokens_total[5m]))  # Generation tokens/s

# Resource allocation per model pod (CPU requests/limits)
kube_pod_container_resource_requests{namespace="llm", resource="cpu", container="main"}
kube_pod_container_resource_limits{namespace="llm", resource="cpu", container="main"}

# Resource allocation per model pod (Memory requests/limits)
kube_pod_container_resource_requests{namespace="llm", resource="memory", container="main"}
kube_pod_container_resource_limits{namespace="llm", resource="memory", container="main"}

# Requests & errors per user (authorized vs rate-limited)
sum by (user) (rate(authorized_calls[5m]))
sum by (user) (rate(limited_calls[5m]))
```

## ğŸ”— Related Files

- **TelemetryPolicy**: `deployment/base/observability/telemetry-policy.yaml`
- **ServiceMonitors**: `deployment/components/observability/prometheus/`

## ğŸ“Š Dashboard Panels

### Platform Admin Dashboard

**Variables (dropdown selectors):**
- `Datasource` - Prometheus datasource
- `MaaS Namespace` - Filter by namespace (default: All)
- `Model` - Filter model metrics by model name (default: All)

**Sections:**

| Section | Panels |
| ------- | ------ |
| **ğŸ¥ Component Health** | Limitador status, Authorino status, MaaS API pods, Gateway pods |
| **ğŸš¨ Alerts** | Firing alerts count, Active alerts table (MaaS namespaces only) |
| **ğŸ“Š Key Metrics** | Total authorized hits, Current rate, Success rate, Active users, P50 latency |
| **ğŸ“ˆ Traffic Analysis** | Request rate by model, Overall error rate (4xx/5xx/rate-limited), Request rate by tier, P95 latency by service |
| **ğŸ† Top Users** | Top 10 by hits, Top 10 by declined requests |
| **ğŸ¤– Model Metrics** | Requests running, Requests waiting, GPU cache usage, Total requests, Model queue depth, Model inference latency (P50/P95/P99) |
| **ğŸ”¤ Token Metrics** | Tokens (1h), Token throughput (works with simulator and real vLLM) |
| **ğŸ“¦ Resource Allocation** | Resource allocation per model table (CPU/Memory requests/limits) |
| **ğŸ‘¤ User Tracking** | Requests & errors per user (authorized vs rate-limited) |
| **ğŸ“‹ Detailed Breakdown** | Request rate by user, Request volume by user/model/tier |
| **ğŸ”® Blocked Features** | Latency per user (placeholder), Token consumption per user (placeholder), Implementation notes |

### AI Engineer Dashboard
- **User-filtered views**: Per-user request volumes and rate limiting

## ğŸ“ Notes

- Dashboards are compatible with Kuadrant v1.2.0+ (with custom Limitador build)
- âœ… Per-user, per-model, per-tier filtering is fully working
- âœ… P50/P95/P99 latency from Istio gateway histograms
- âœ… Error tracking (401, 429, 5xx) from Istio + Limitador
- âœ… Alert integration (MaaS-filtered firing alerts)
- âœ… vLLM/KServe model metrics (queue depth, GPU cache, inference latency)
- âœ… Model selector dropdown to filter model metrics
- âœ… **Resource allocation per model** - CPU/Memory requests/limits from kube-state-metrics
- âœ… **Requests & errors per user** - authorized vs rate-limited from Limitador
- âœ… Token metrics work with both real vLLM (`vllm:prompt_tokens_total`) and simulator (`vllm:request_prompt_tokens_sum`) - dashboards use `OR` queries for compatibility
- âš ï¸ Model latency histograms only appear after traffic is generated (lazy-initialized)
- âŒ **Latency per user** - Blocked: Istio metrics don't include `user` label (requires EnvoyFilter)
- âŒ **Token consumption per user** - Blocked: vLLM doesn't label metrics with `user` (requires vLLM changes)
- Requires Prometheus Operator for ServiceMonitor support
- Dashboard auto-refreshes every 30 seconds

To customize the dashboard:
1. Import into Grafana
2. Edit panels as needed
3. Export updated JSON
4. Replace this file with your custom version
